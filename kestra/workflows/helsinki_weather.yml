id: weather_daily_pipeline
namespace: weather
description: "Daily weather data pipeline for Helsinki using archive API"


variables:
  api_base_url: "https://api.open-meteo.com/v1/archive"
  location_latitude: "60.1695"  # Helsinki
  location_longitude: "24.9354"  # Helsinki
  timezone: "Europe/Helsinki"
  hourly_params: "temperature_2m,rain,snowfall,weather_code,soil_temperature_0_to_7cm,relative_humidity_2m,precipitation"
  gcp_project: "forward-dream-451119-q3"
  bigquery_dataset: "weather_dataset"
  bigquery_raw_table: "weather_data"


tasks:
  - id: calculate_date_range
    type: io.kestra.plugin.scripts.python.Script
    description: "Calculate start date for API call (fetch yesterday's data)"
    runner: PROCESS
    script: |
      import datetime
      from kestra import Kestra

      today = datetime.datetime.now()
      yesterday = today - datetime.timedelta(days=1)
      before_yesterday=yesterday-datetime.timedelta(days=1)
      #start_date = before_yesterday.strftime('%Y-%m-%d')  # '2025-04-07' etc
      #end_date=yesterday.strftime('%Y-%m-%d')
      search_date= before_yesterday.strftime('%Y-%m-%d')

      # we put the variables to output
      Kestra.outputs({
          "search_day": search_day
          #"start_date": start_date,
          #"end_date": end_date
      })


  - id: print_output
    type: io.kestra.plugin.core.log.Log
    message: |
      Value from Python: {{ outputs.calculate_date_range.vars.start_date }}

  - id: fetch_weather_data
    type: io.kestra.plugin.core.http.Request
    description: "Fetch weather data using dynamic start_date"
    uri: "https://archive-api.open-meteo.com/v1/archive?start_date={{ outputs.calculate_date_range.vars.search_day }}&end_date={{ outputs.calculate_date_range.vars.search_day }}&latitude={{ vars.location_latitude }}&longitude={{ vars.location_longitude }}&hourly={{ vars.hourly_params }}&timezone={{ vars.timezone }}"
    method: GET


  - id: transform_and_load_to_bigquery
    type: io.kestra.plugin.scripts.python.Script
    description: "Transform API response and load to BigQuery"
    runner: PROCESS 
    inputFiles:
      fetch_weather_data.json: "{{ outputs.fetch_weather_data }}"
    env:
      GOOGLE_APPLICATION_CREDENTIALS: /app/.dbt/my-creds.json
    beforeCommands:
     - pip install google-cloud-bigquery pandas-gbq --upgrade  --no-warn-script-location pyarrow --no-cache-dir
    script: |
      import json
      import pandas as pd
      from google.cloud import bigquery
      import os
      from datetime import datetime
      
      # Dosyadan API yanıtını oku
      with open('fetch_weather_data.json', 'r') as f:
          content = f.read()
      
      # API yanıtını doğrudan yazdır
      print("Raw API response:", content[:200] + "..." if len(content) > 200 else content)
      
      # JSON'ı parse et
      response_data = json.loads(content)
      
      # Body içeriğini al
      body_content = response_data.get('body', '{}')
      
      # Body içeriğini yazdır
      print("Body content (first 200 chars):", body_content[:200] + "..." if len(body_content) > 200 else body_content)
      
      # Body içeriğini JSON olarak parse et
      try:
          weather_data = json.loads(body_content)
          print("Weather data keys:", weather_data.keys())
          
          # Hourly verisi var mı kontrol et
          if 'hourly' in weather_data:
              print("Hourly data keys:", weather_data['hourly'].keys())
              print("Time data length:", len(weather_data['hourly']['time']))
              
              # Veriyi çıkar
              times = weather_data['hourly']['time']
              temperature = weather_data['hourly']['temperature_2m']
              rain = weather_data['hourly']['rain']
              snowfall = weather_data['hourly']['snowfall']
              weather_code = weather_data['hourly']['weather_code']
              soil_temp = weather_data['hourly']['soil_temperature_0_to_7cm']
              rel_humidity = weather_data['hourly']['relative_humidity_2m']
              precipitation = weather_data['hourly']['precipitation']
              
              # DataFrame oluştur
              df = pd.DataFrame({
                  "date_str": times,
                  "temperature_2m": temperature,
                  "rain": rain,
                  "snowfall": snowfall,
                  "weather_code": weather_code, 
                  "soil_temperature_0_to_7cm": soil_temp,
                  "relative_humidity_2m": rel_humidity,
                  "precipitation": precipitation
              })
              
              print("DataFrame created with shape:", df.shape)
              print("DataFrame head:")
              print(df.head().to_string())
              
              # Tarihleri uygun formata dönüştür
              df['date'] = pd.to_datetime(df['date_str'])
              df = df.drop(columns=['date_str'])
              
              # Veri tipi dönüşümlerini kontrol et
              #df['weather_code'] = df['weather_code'].astype(int)
              
              # BigQuery config
              client = bigquery.Client(project="forward-dream-451119-q3")
              table_id = "forward-dream-451119-q3.weather_dataset.weather_data"
              
              # Define schema for BigQuery table
              schema = [
                  bigquery.SchemaField("date", "TIMESTAMP"),
                  bigquery.SchemaField("temperature_2m", "FLOAT"),
                  bigquery.SchemaField("rain", "FLOAT"),
                  bigquery.SchemaField("snowfall", "FLOAT"),
                  bigquery.SchemaField("weather_code", "FLOAT"),
                  bigquery.SchemaField("soil_temperature_0_to_7cm", "FLOAT"),
                  bigquery.SchemaField("relative_humidity_2m", "FLOAT"),
                  bigquery.SchemaField("precipitation", "FLOAT")
              ]
              
              # BigQuery job configuration
              job_config = bigquery.LoadJobConfig(
                  write_disposition="WRITE_APPEND",
                  schema=schema  # Applying schema for proper column mapping
              )
              
              try:
                  # Load DataFrame to BigQuery
                  job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
                  job_result = job.result()  # Wait for the job to complete
                  print(f"Successfully inserted {len(df)} rows into {table_id}")
              except Exception as e:
                  print(f"Error loading data to BigQuery: {e}")
                  import traceback
                  traceback.print_exc()
          else:
              print("No 'hourly' data found in the API response!")
              print("Available keys:", weather_data.keys())
      except json.JSONDecodeError as e:
          print(f"Error parsing body content as JSON: {e}")
          print("Raw body content:", body_content)
 

  - id: run_dbt
    type: io.kestra.plugin.scripts.shell.Commands
    runner: PROCESS 
    commands:
      - cd /app/workflows/dbt_project
      - dbt run --profiles-dir /app/.dbt --log-path /tmp/dbt_logs --target prod
      

triggers:
  - id: daily_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 0 * * *"  # Work on everyday 00:00 
