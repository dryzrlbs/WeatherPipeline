
id: weather_daily_pipeline
namespace: weather
description: "Daily weather data pipeline for Helsinki using archive API"

variables:
  api_base_url: "https://api.open-meteo.com/v1/archive"
  location_latitude: "60.1695"  # Helsinki
  location_longitude: "24.9354"  # Helsinki
  timezone: "Europe/Berlin"
  hourly_params: "temperature_2m,rain,snowfall,weather_code,soil_temperature_0_to_7cm,relative_humidity_2m,precipitation"
  gcp_project: "forward-dream-451119-q3"
  bigquery_dataset: "weather_dataset"
  bigquery_raw_table: "weather_data"

tasks:
  - id: calculate_date_range
    type: io.kestra.plugin.scripts.python.Script
    description: "Calculate date range for API call (fetch yesterday's data)"
    runner: PROCESS
    script: |
      import datetime
      import json

      today = datetime.datetime.now()
      yesterday = today - datetime.timedelta(days=1)
      start_date = yesterday.strftime('%Y-%m-%d')
      end_date = today.strftime('%Y-%m-%d')

      result = {
          "start_date": start_date,
          "end_date": end_date
      }
      print(json.dumps(result))
      with open("/app/response.json", "w") as f:
       json.dump(result, f)

  - id: fetch_weather_data
    type: io.kestra.plugin.core.http.Request
    description: "Fetch hourly weather data from Open Meteo Archive API for Helsinki"
    uri: "https://archive-api.open-meteo.com/v1/archive?start_date=2025-04-05&end_date=2025-04-06&latitude={{ vars.location_latitude }}&longitude={{ vars.location_longitude }}&hourly={{ vars.hourly_params }}&timezone={{ vars.timezone }}"
    method: GET

  
  
  
  - id: transform_and_load_to_bigquery
    type: io.kestra.plugin.scripts.python.Script
    description: "Transform API response and load to BigQuery"
    env:
      GOOGLE_APPLICATION_CREDENTIALS: /app/secrets/my-creds.json
    beforeCommands:
      - pip install google-cloud-bigquery pandas --no-cache-dir
    script: |
      import json
      import pandas as pd
      from google.cloud import bigquery
      import os

    
      # Extract data from the API response
      times = api_data.get('hourly', {}).get('time', [])
      temperature = api_data.get('hourly', {}).get('temperature_2m', [])
      rain = api_data.get('hourly', {}).get('rain', [])
      snowfall = api_data.get('hourly', {}).get('snowfall', [])
      weather_code = api_data.get('hourly', {}).get('weather_code', [])
      soil_temp = api_data.get('hourly', {}).get('soil_temperature_0_to_7cm', [])
      rel_humidity = api_data.get('hourly', {}).get('relative_humidity_2m', [])
      precipitation = api_data.get('hourly', {}).get('precipitation', [])

      # Create DataFrame
      df = pd.DataFrame({
        "date": times,
        "temperature_2m": temperature,
        "rain": rain,
        "snowfall": snowfall,
        "weather_code": weather_code,
        "soil_temperature_0_to_7cm": soil_temp,
        "relative_humidity_2m": rel_humidity,
        "precipitation": precipitation
      })

      # BigQuery config
      client = bigquery.Client(project="forward-dream-451119-q3")
      table_id = "forward-dream-451119-q3.weather_dataset.weather_data"

      # Define schema for BigQuery table
      schema = [
          bigquery.SchemaField("date", "TIMESTAMP"),
          bigquery.SchemaField("temperature_2m", "FLOAT"),
          bigquery.SchemaField("rain", "FLOAT"),
          bigquery.SchemaField("snowfall", "FLOAT"),
          bigquery.SchemaField("weather_code", "STRING"),
          bigquery.SchemaField("soil_temperature_0_to_7cm", "FLOAT"),
          bigquery.SchemaField("relative_humidity_2m", "FLOAT"),
          bigquery.SchemaField("precipitation", "FLOAT")
      ]

      # BigQuery job configuration
      job_config = bigquery.LoadJobConfig(
          write_disposition="WRITE_APPEND",
          schema=schema  # Applying schema for proper column mapping
      )

      # Load DataFrame to BigQuery
      job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
      job.result()  # Wait for the job to complete

      print(f"Inserted {len(df)} rows into {table_id}")






  - id: transform_and_load_to_bigquery
    type: io.kestra.plugin.scripts.python.Script
    description: "Transform API response and load to BigQuery"
    runner: PROCESS 
    inputs:
    - name: fetch_weather_data # Önceki görevin ID'si
      type: JSON # API yanıtı JSON formatında
    env:
      GOOGLE_APPLICATION_CREDENTIALS: /app/secrets/my-creds.json
    beforeCommands:
      - pip install google-cloud-bigquery pandas --no-cache-dir
    script: |
      import json
      import pandas as pd
      from google.cloud import bigquery
      import os
      from kestra import Kestra
    
      # Önceki görevden gelen API yanıtını al
      api_data_raw = Kestra.inputs["fetch_weather_data"]
      api_data = json.loads(api_data_raw)
      # Extract data from the API response
      times = api_data.get('hourly', {}).get('time', [])
      # ... geri kalan kod aynı ...









  - id: transform_and_load_to_bigquery
    type: io.kestra.plugin.scripts.python.Script
    description: "Transform API response and load to BigQuery"
    runner: PROCESS
    inputFiles:
     fetch_weather_data.json: "{{ outputs.fetch_weather_data }}"
    env:
      GOOGLE_APPLICATION_CREDENTIALS: /app/secrets/my-creds.json
    beforeCommands:
      - pip install google-cloud-bigquery pandas --no-cache-dir
    script: |
      import json
      import pandas as pd
      from google.cloud import bigquery
      import os
      from kestra import Kestra
    
      # Önceki görevden gelen API yanıtını al
      api_data_raw = Kestra.inputs["fetch_weather_data"]
      api_data = json.loads(api_data_raw)
    
      # Extract data from the API response
      times = api_data.get('hourly', {}).get('time', [])
      temperature = api_data.get('hourly', {}).get('temperature_2m', [])
      rain = api_data.get('hourly', {}).get('rain', [])
      snowfall = api_data.get('hourly', {}).get('snowfall', [])
      weather_code = api_data.get('hourly', {}).get('weather_code', [])
      soil_temp = api_data.get('hourly', {}).get('soil_temperature_0_to_7cm', [])
      rel_humidity = api_data.get('hourly', {}).get('relative_humidity_2m', [])
      precipitation = api_data.get('hourly', {}).get('precipitation', [])

      # Create DataFrame
      df = pd.DataFrame({
        "date": times,
        "temperature_2m": temperature,
        "rain": rain,
        "snowfall": snowfall,
        "weather_code": weather_code,
        "soil_temperature_0_to_7cm": soil_temp,
        "relative_humidity_2m": rel_humidity,
        "precipitation": precipitation
      })

      # BigQuery config
      client = bigquery.Client(project="forward-dream-451119-q3")
      table_id = "forward-dream-451119-q3.weather_dataset.weather_data"

      # Define schema for BigQuery table
      schema = [
          bigquery.SchemaField("date", "TIMESTAMP"),
          bigquery.SchemaField("temperature_2m", "FLOAT"),
          bigquery.SchemaField("rain", "FLOAT"),
          bigquery.SchemaField("snowfall", "FLOAT"),
          bigquery.SchemaField("weather_code", "STRING"),
          bigquery.SchemaField("soil_temperature_0_to_7cm", "FLOAT"),
          bigquery.SchemaField("relative_humidity_2m", "FLOAT"),
          bigquery.SchemaField("precipitation", "FLOAT")
      ]

      # BigQuery job configuration
      job_config = bigquery.LoadJobConfig(
          write_disposition="WRITE_APPEND",
          schema=schema  # Applying schema for proper column mapping
      )

      # Load DataFrame to BigQuery
      job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
      job.result()  # Wait for the job to complete

      print(f"Inserted {len(df)} rows into {table_id}")



-----
YEDEK08042025 10:37


id: weather_daily_pipeline
namespace: weather
description: "Daily weather data pipeline for Helsinki using archive API"

variables:
  api_base_url: "https://api.open-meteo.com/v1/archive"
  location_latitude: "60.1695"  # Helsinki
  location_longitude: "24.9354"  # Helsinki
  timezone: "Europe/Berlin"
  hourly_params: "temperature_2m,rain,snowfall,weather_code,soil_temperature_0_to_7cm,relative_humidity_2m,precipitation"
  gcp_project: "forward-dream-451119-q3"
  bigquery_dataset: "weather_dataset"
  bigquery_raw_table: "weather_data"

tasks:
  - id: calculate_date_range
    type: io.kestra.plugin.scripts.python.Script
    description: "Calculate date range for API call (fetch yesterday's data)"
    runner: PROCESS
    script: |
      import datetime
      import json

      today = datetime.datetime.now()
      yesterday = today - datetime.timedelta(days=1)
      start_date = yesterday.strftime('%Y-%m-%d')
      end_date = today.strftime('%Y-%m-%d')

      result = {
          "start_date": start_date,
          "end_date": end_date
      }

  - id: fetch_weather_data
    type: io.kestra.plugin.core.http.Request
    description: "Fetch hourly weather data from Open Meteo Archive API for Helsinki"
    uri: "https://archive-api.open-meteo.com/v1/archive?start_date=2025-04-04&end_date=2025-04-05&latitude={{ vars.location_latitude }}&longitude={{ vars.location_longitude }}&hourly={{ vars.hourly_params }}&timezone={{ vars.timezone }}"
    method: GET


  - id: transform_and_load_to_bigquery
    type: io.kestra.plugin.scripts.python.Script
    description: "Transform API response and load to BigQuery"
    runner: PROCESS 
    inputFiles:
      fetch_weather_data.json: "{{ outputs.fetch_weather_data }}"
    env:
      GOOGLE_APPLICATION_CREDENTIALS: /app/secrets/my-creds.json
    beforeCommands:
     - pip install google-cloud-bigquery pandas-gbq --upgrade  --no-warn-script-location pyarrow --no-cache-dir
    script: |
      import json
      import pandas as pd
      from google.cloud import bigquery
      import os
      from datetime import datetime
      

      # Dosyadan API yanıtını oku
      with open('fetch_weather_data.json', 'r') as f:
        content = f.read()
    
        # API yanıtının biçimine göre uyarla
      if content.startswith("{\"body\":"):
        # JSON String'inden body kısmını ayıkla
        try:
            wrapper = json.loads(content)
            body_str = wrapper.get("body", "{}")
            # Body içeriği de bir JSON string'i olabilir
            api_data = json.loads(body_str)
        except json.JSONDecodeError as e:
            print(f"JSON parsing error: {e}")
            api_data = {}
      else:
        # Doğrudan JSON olarak okuma dene
        try:
            api_data = json.loads(content)
        except json.JSONDecodeError as e:
            print(f"JSON parsing error: {e}")
            api_data = {}
    
      # API yanıtını kontrol et
      print(f"API data keys: {api_data.keys() if isinstance(api_data, dict) else 'Not a dictionary'}")
      if 'hourly' in api_data:
       print(f"Hourly keys: {api_data['hourly'].keys()}")




      # Extract data from the API response
      times = api_data.get('hourly', {}).get('time', [])
      temperature = api_data.get('hourly', {}).get('temperature_2m', [])
      rain = api_data.get('hourly', {}).get('rain', [])
      snowfall = api_data.get('hourly', {}).get('snowfall', [])
      weather_code = api_data.get('hourly', {}).get('weather_code', [])
      soil_temp = api_data.get('hourly', {}).get('soil_temperature_0_to_7cm', [])
      rel_humidity = api_data.get('hourly', {}).get('relative_humidity_2m', [])
      precipitation = api_data.get('hourly', {}).get('precipitation', [])

      # Create DataFrame
      df = pd.DataFrame({
        "date": times,
        "temperature_2m": temperature,
        "rain": rain,
        "snowfall": snowfall,
        "weather_code": weather_code,
        "soil_temperature_0_to_7cm": soil_temp,
        "relative_humidity_2m": rel_humidity,
        "precipitation": precipitation
      })

      
      df.date = pd.to_datetime(df.date)
      # Tarihleri uygun formata dönüştür
      print(df.shape)
      print(df.head())
      print(df.dtypes)

      # BigQuery config
      client = bigquery.Client(project="forward-dream-451119-q3")
      table_id = "forward-dream-451119-q3.weather_dataset.weather_data"

      # Define schema for BigQuery table
      schema = [
          bigquery.SchemaField("date", "TIMESTAMP"),
          bigquery.SchemaField("temperature_2m", "FLOAT"),
          bigquery.SchemaField("rain", "FLOAT"),
          bigquery.SchemaField("snowfall", "FLOAT"),
          bigquery.SchemaField("weather_code", "FLOAT"),
          bigquery.SchemaField("soil_temperature_0_to_7cm", "FLOAT"),
          bigquery.SchemaField("relative_humidity_2m", "FLOAT"),
          bigquery.SchemaField("precipitation", "FLOAT")
      ]

      # BigQuery job configuration
      job_config = bigquery.LoadJobConfig(
          write_disposition="WRITE_APPEND", autodetect=True
            # Applying schema for proper column mapping    schema=schema
      )

      # Load DataFrame to BigQuery
      job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
      job.result()  # Wait for the job to complete

      print(f"Inserted {len(df)} rows into {table_id}")

      
*******************************************************************

YEDEK CALISAN 10:48 sadece tarihler kaldi yapilacak


id: weather_daily_pipeline
namespace: weather
description: "Daily weather data pipeline for Helsinki using archive API"

variables:
  api_base_url: "https://api.open-meteo.com/v1/archive"
  location_latitude: "60.1695"  # Helsinki
  location_longitude: "24.9354"  # Helsinki
  timezone: "Europe/Berlin"
  hourly_params: "temperature_2m,rain,snowfall,weather_code,soil_temperature_0_to_7cm,relative_humidity_2m,precipitation"
  gcp_project: "forward-dream-451119-q3"
  bigquery_dataset: "weather_dataset"
  bigquery_raw_table: "weather_data"

tasks:
  - id: calculate_date_range
    type: io.kestra.plugin.scripts.python.Script
    description: "Calculate date range for API call (fetch yesterday's data)"
    runner: PROCESS
    script: |
      import datetime
      import json

      today = datetime.datetime.now()
      yesterday = today - datetime.timedelta(days=1)
      start_date = yesterday.strftime('%Y-%m-%d')
      end_date = today.strftime('%Y-%m-%d')

      result = {
          "start_date": start_date,
          "end_date": end_date
      }

  - id: fetch_weather_data
    type: io.kestra.plugin.core.http.Request
    description: "Fetch hourly weather data from Open Meteo Archive API for Helsinki"
    uri: "https://archive-api.open-meteo.com/v1/archive?start_date=2025-04-04&end_date=2025-04-05&latitude={{ vars.location_latitude }}&longitude={{ vars.location_longitude }}&hourly={{ vars.hourly_params }}&timezone={{ vars.timezone }}"
    method: GET


  - id: transform_and_load_to_bigquery
    type: io.kestra.plugin.scripts.python.Script
    description: "Transform API response and load to BigQuery"
    runner: PROCESS 
    inputFiles:
      fetch_weather_data.json: "{{ outputs.fetch_weather_data }}"
    env:
      GOOGLE_APPLICATION_CREDENTIALS: /app/secrets/my-creds.json
    beforeCommands:
     - pip install google-cloud-bigquery pandas-gbq --upgrade  --no-warn-script-location pyarrow --no-cache-dir
    script: |
      import json
      import pandas as pd
      from google.cloud import bigquery
      import os
      from datetime import datetime
      
      # Dosyadan API yanıtını oku
      with open('fetch_weather_data.json', 'r') as f:
          content = f.read()
      
      # API yanıtını doğrudan yazdır
      print("Raw API response:", content[:200] + "..." if len(content) > 200 else content)
      
      # JSON'ı parse et
      response_data = json.loads(content)
      
      # Body içeriğini al
      body_content = response_data.get('body', '{}')
      
      # Body içeriğini yazdır
      print("Body content (first 200 chars):", body_content[:200] + "..." if len(body_content) > 200 else body_content)
      
      # Body içeriğini JSON olarak parse et
      try:
          weather_data = json.loads(body_content)
          print("Weather data keys:", weather_data.keys())
          
          # Hourly verisi var mı kontrol et
          if 'hourly' in weather_data:
              print("Hourly data keys:", weather_data['hourly'].keys())
              print("Time data length:", len(weather_data['hourly']['time']))
              
              # Veriyi çıkar
              times = weather_data['hourly']['time']
              temperature = weather_data['hourly']['temperature_2m']
              rain = weather_data['hourly']['rain']
              snowfall = weather_data['hourly']['snowfall']
              weather_code = weather_data['hourly']['weather_code']
              soil_temp = weather_data['hourly']['soil_temperature_0_to_7cm']
              rel_humidity = weather_data['hourly']['relative_humidity_2m']
              precipitation = weather_data['hourly']['precipitation']
              
              # DataFrame oluştur
              df = pd.DataFrame({
                  "date_str": times,
                  "temperature_2m": temperature,
                  "rain": rain,
                  "snowfall": snowfall,
                  "weather_code": weather_code, 
                  "soil_temperature_0_to_7cm": soil_temp,
                  "relative_humidity_2m": rel_humidity,
                  "precipitation": precipitation
              })
              
              print("DataFrame created with shape:", df.shape)
              print("DataFrame head:")
              print(df.head().to_string())
              
              # Tarihleri uygun formata dönüştür
              df['date'] = pd.to_datetime(df['date_str'])
              df = df.drop(columns=['date_str'])
              
              # Veri tipi dönüşümlerini kontrol et
              #df['weather_code'] = df['weather_code'].astype(int)
              
              # BigQuery config
              client = bigquery.Client(project="forward-dream-451119-q3")
              table_id = "forward-dream-451119-q3.weather_dataset.weather_data"
              
              # Define schema for BigQuery table
              schema = [
                  bigquery.SchemaField("date", "TIMESTAMP"),
                  bigquery.SchemaField("temperature_2m", "FLOAT"),
                  bigquery.SchemaField("rain", "FLOAT"),
                  bigquery.SchemaField("snowfall", "FLOAT"),
                  bigquery.SchemaField("weather_code", "FLOAT"),
                  bigquery.SchemaField("soil_temperature_0_to_7cm", "FLOAT"),
                  bigquery.SchemaField("relative_humidity_2m", "FLOAT"),
                  bigquery.SchemaField("precipitation", "FLOAT")
              ]
              
              # BigQuery job configuration
              job_config = bigquery.LoadJobConfig(
                  write_disposition="WRITE_APPEND",
                  schema=schema  # Applying schema for proper column mapping
              )
              
              try:
                  # Load DataFrame to BigQuery
                  job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
                  job_result = job.result()  # Wait for the job to complete
                  print(f"Successfully inserted {len(df)} rows into {table_id}")
              except Exception as e:
                  print(f"Error loading data to BigQuery: {e}")
                  import traceback
                  traceback.print_exc()
          else:
              print("No 'hourly' data found in the API response!")
              print("Available keys:", weather_data.keys())
      except json.JSONDecodeError as e:
          print(f"Error parsing body content as JSON: {e}")
          print("Raw body content:", body_content)
********************************
      
    outputs:
      start_date: "{{ outputs.calculate_date_range.start_date.value }}"
      end_date: "{{ outputs.calculate_date_range.end_date.value }}"
    

************************************************
CALISAN KESTRA


id: pass-data-between-tasks
namespace: company.team

variables:
  api_base_url: "https://api.open-meteo.com/v1/archive"
  location_latitude: "60.1695"  # Helsinki
  location_longitude: "24.9354"  # Helsinki
  timezone: "Europe/Berlin"
  hourly_params: "temperature_2m,rain,snowfall,weather_code,soil_temperature_0_to_7cm,relative_humidity_2m,precipitation"
  gcp_project: "forward-dream-451119-q3"
  bigquery_dataset: "weather_dataset"
  bigquery_raw_table: "weather_data"


tasks:
  - id: calculate_date_range
    type: io.kestra.plugin.scripts.python.Script
    description: "Calculate start date for API call (fetch yesterday's data)"
    runner: PROCESS
    script: |
      import datetime
      from kestra import Kestra

      today = datetime.datetime.now()
      yesterday = today - datetime.timedelta(days=1)
      before_yesterday=yesterday-datetime.timedelta(days=1)
      start_date = before_yesterday.strftime('%Y-%m-%d')  # '2025-04-07' gibi
      end_date=yesterday.strftime('%Y-%m-%d')

      # Çıktıyı vars değişkenine yazıyoruz
      Kestra.outputs({
          "start_date": start_date,
          "end_date": end_date
      })

  - id: print_output
    type: io.kestra.plugin.core.log.Log
    message: |
      Value from Python: {{ outputs.calculate_date_range.vars.start_date }}

  - id: fetch_weather_data
    type: io.kestra.plugin.core.http.Request
    description: "Fetch weather data using dynamic start_date"
    uri: "https://archive-api.open-meteo.com/v1/archive?start_date={{ outputs.calculate_date_range.vars.start_date }}&end_date={{ outputs.calculate_date_range.vars.end_date }}&latitude={{ vars.location_latitude }}&longitude={{ vars.location_longitude }}&hourly={{ vars.hourly_params }}&timezone={{ vars.timezone }}"
    method: GET

  - id: transform_and_load_to_bigquery
    type: io.kestra.plugin.scripts.python.Script
    description: "Transform API response and load to BigQuery"
    runner: PROCESS 
    inputFiles:
      fetch_weather_data.json: "{{ outputs.fetch_weather_data }}"
    env:
      GOOGLE_APPLICATION_CREDENTIALS: /app/secrets/my-creds.json
    beforeCommands:
     - pip install google-cloud-bigquery pandas-gbq --upgrade  --no-warn-script-location pyarrow --no-cache-dir
    script: |
      import json
      import pandas as pd
      from google.cloud import bigquery
      import os
      from datetime import datetime
      
      # Dosyadan API yanıtını oku
      with open('fetch_weather_data.json', 'r') as f:
          content = f.read()
      
      # API yanıtını doğrudan yazdır
      print("Raw API response:", content[:200] + "..." if len(content) > 200 else content)
      
      # JSON'ı parse et
      response_data = json.loads(content)
      
      # Body içeriğini al
      body_content = response_data.get('body', '{}')
      
      # Body içeriğini yazdır
      print("Body content (first 200 chars):", body_content[:200] + "..." if len(body_content) > 200 else body_content)
      
      # Body içeriğini JSON olarak parse et
      try:
          weather_data = json.loads(body_content)
          print("Weather data keys:", weather_data.keys())
          
          # Hourly verisi var mı kontrol et
          if 'hourly' in weather_data:
              print("Hourly data keys:", weather_data['hourly'].keys())
              print("Time data length:", len(weather_data['hourly']['time']))
              
              # Veriyi çıkar
              times = weather_data['hourly']['time']
              temperature = weather_data['hourly']['temperature_2m']
              rain = weather_data['hourly']['rain']
              snowfall = weather_data['hourly']['snowfall']
              weather_code = weather_data['hourly']['weather_code']
              soil_temp = weather_data['hourly']['soil_temperature_0_to_7cm']
              rel_humidity = weather_data['hourly']['relative_humidity_2m']
              precipitation = weather_data['hourly']['precipitation']
              
              # DataFrame oluştur
              df = pd.DataFrame({
                  "date_str": times,
                  "temperature_2m": temperature,
                  "rain": rain,
                  "snowfall": snowfall,
                  "weather_code": weather_code, 
                  "soil_temperature_0_to_7cm": soil_temp,
                  "relative_humidity_2m": rel_humidity,
                  "precipitation": precipitation
              })
              
              print("DataFrame created with shape:", df.shape)
              print("DataFrame head:")
              print(df.head().to_string())
              
              # Tarihleri uygun formata dönüştür
              df['date'] = pd.to_datetime(df['date_str'])
              df = df.drop(columns=['date_str'])
              
              # Veri tipi dönüşümlerini kontrol et
              #df['weather_code'] = df['weather_code'].astype(int)
              
              # BigQuery config
              client = bigquery.Client(project="forward-dream-451119-q3")
              table_id = "forward-dream-451119-q3.weather_dataset.weather_data"
              
              # Define schema for BigQuery table
              schema = [
                  bigquery.SchemaField("date", "TIMESTAMP"),
                  bigquery.SchemaField("temperature_2m", "FLOAT"),
                  bigquery.SchemaField("rain", "FLOAT"),
                  bigquery.SchemaField("snowfall", "FLOAT"),
                  bigquery.SchemaField("weather_code", "FLOAT"),
                  bigquery.SchemaField("soil_temperature_0_to_7cm", "FLOAT"),
                  bigquery.SchemaField("relative_humidity_2m", "FLOAT"),
                  bigquery.SchemaField("precipitation", "FLOAT")
              ]
              
              # BigQuery job configuration
              job_config = bigquery.LoadJobConfig(
                  write_disposition="WRITE_APPEND",
                  schema=schema  # Applying schema for proper column mapping
              )
              
              try:
                  # Load DataFrame to BigQuery
                  job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
                  job_result = job.result()  # Wait for the job to complete
                  print(f"Successfully inserted {len(df)} rows into {table_id}")
              except Exception as e:
                  print(f"Error loading data to BigQuery: {e}")
                  import traceback
                  traceback.print_exc()
          else:
              print("No 'hourly' data found in the API response!")
              print("Available keys:", weather_data.keys())
      except json.JSONDecodeError as e:
          print(f"Error parsing body content as JSON: {e}")
          print("Raw body content:", body_content)



**********************************CALISAN 21:12 08/04/2025

id: weather_daily_pipeline
namespace: weather
description: "Daily weather data pipeline for Helsinki using archive API"


variables:
  api_base_url: "https://api.open-meteo.com/v1/archive"
  location_latitude: "60.1695"  # Helsinki
  location_longitude: "24.9354"  # Helsinki
  timezone: "Europe/Berlin"
  hourly_params: "temperature_2m,rain,snowfall,weather_code,soil_temperature_0_to_7cm,relative_humidity_2m,precipitation"
  gcp_project: "forward-dream-451119-q3"
  bigquery_dataset: "weather_dataset"
  bigquery_raw_table: "weather_data"


tasks:
  - id: calculate_date_range
    type: io.kestra.plugin.scripts.python.Script
    description: "Calculate start date for API call (fetch yesterday's data)"
    runner: PROCESS
    script: |
      import datetime
      from kestra import Kestra

      today = datetime.datetime.now()
      yesterday = today - datetime.timedelta(days=1)
      before_yesterday=yesterday-datetime.timedelta(days=1)
      start_date = before_yesterday.strftime('%Y-%m-%d')  # '2025-04-07' gibi
      end_date=yesterday.strftime('%Y-%m-%d')

      # Çıktıyı vars değişkenine yazıyoruz
      Kestra.outputs({
          "start_date": start_date,
          "end_date": end_date
      })


  - id: print_output
    type: io.kestra.plugin.core.log.Log
    message: |
      Value from Python: {{ outputs.calculate_date_range.vars.start_date }}

  - id: fetch_weather_data
    type: io.kestra.plugin.core.http.Request
    description: "Fetch weather data using dynamic start_date"
    uri: "https://archive-api.open-meteo.com/v1/archive?start_date={{ outputs.calculate_date_range.vars.start_date }}&end_date={{ outputs.calculate_date_range.vars.end_date }}&latitude={{ vars.location_latitude }}&longitude={{ vars.location_longitude }}&hourly={{ vars.hourly_params }}&timezone={{ vars.timezone }}"
    method: GET

  - id: transform_and_load_to_bigquery
    type: io.kestra.plugin.scripts.python.Script
    description: "Transform API response and load to BigQuery"
    runner: PROCESS 
    inputFiles:
      fetch_weather_data.json: "{{ outputs.fetch_weather_data }}"
    env:
      GOOGLE_APPLICATION_CREDENTIALS: /app/secrets/my-creds.json
    beforeCommands:
     - pip install google-cloud-bigquery pandas-gbq --upgrade  --no-warn-script-location pyarrow --no-cache-dir
    script: |
      import json
      import pandas as pd
      from google.cloud import bigquery
      import os
      from datetime import datetime
      
      # Dosyadan API yanıtını oku
      with open('fetch_weather_data.json', 'r') as f:
          content = f.read()
      
      # API yanıtını doğrudan yazdır
      print("Raw API response:", content[:200] + "..." if len(content) > 200 else content)
      
      # JSON'ı parse et
      response_data = json.loads(content)
      
      # Body içeriğini al
      body_content = response_data.get('body', '{}')
      
      # Body içeriğini yazdır
      print("Body content (first 200 chars):", body_content[:200] + "..." if len(body_content) > 200 else body_content)
      
      # Body içeriğini JSON olarak parse et
      try:
          weather_data = json.loads(body_content)
          print("Weather data keys:", weather_data.keys())
          
          # Hourly verisi var mı kontrol et
          if 'hourly' in weather_data:
              print("Hourly data keys:", weather_data['hourly'].keys())
              print("Time data length:", len(weather_data['hourly']['time']))
              
              # Veriyi çıkar
              times = weather_data['hourly']['time']
              temperature = weather_data['hourly']['temperature_2m']
              rain = weather_data['hourly']['rain']
              snowfall = weather_data['hourly']['snowfall']
              weather_code = weather_data['hourly']['weather_code']
              soil_temp = weather_data['hourly']['soil_temperature_0_to_7cm']
              rel_humidity = weather_data['hourly']['relative_humidity_2m']
              precipitation = weather_data['hourly']['precipitation']
              
              # DataFrame oluştur
              df = pd.DataFrame({
                  "date_str": times,
                  "temperature_2m": temperature,
                  "rain": rain,
                  "snowfall": snowfall,
                  "weather_code": weather_code, 
                  "soil_temperature_0_to_7cm": soil_temp,
                  "relative_humidity_2m": rel_humidity,
                  "precipitation": precipitation
              })
              
              print("DataFrame created with shape:", df.shape)
              print("DataFrame head:")
              print(df.head().to_string())
              
              # Tarihleri uygun formata dönüştür
              df['date'] = pd.to_datetime(df['date_str'])
              df = df.drop(columns=['date_str'])
              
              # Veri tipi dönüşümlerini kontrol et
              #df['weather_code'] = df['weather_code'].astype(int)
              
              # BigQuery config
              client = bigquery.Client(project="forward-dream-451119-q3")
              table_id = "forward-dream-451119-q3.weather_dataset.weather_data"
              
              # Define schema for BigQuery table
              schema = [
                  bigquery.SchemaField("date", "TIMESTAMP"),
                  bigquery.SchemaField("temperature_2m", "FLOAT"),
                  bigquery.SchemaField("rain", "FLOAT"),
                  bigquery.SchemaField("snowfall", "FLOAT"),
                  bigquery.SchemaField("weather_code", "FLOAT"),
                  bigquery.SchemaField("soil_temperature_0_to_7cm", "FLOAT"),
                  bigquery.SchemaField("relative_humidity_2m", "FLOAT"),
                  bigquery.SchemaField("precipitation", "FLOAT")
              ]
              
              # BigQuery job configuration
              job_config = bigquery.LoadJobConfig(
                  write_disposition="WRITE_APPEND",
                  schema=schema  # Applying schema for proper column mapping
              )
              
              try:
                  # Load DataFrame to BigQuery
                  job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
                  job_result = job.result()  # Wait for the job to complete
                  print(f"Successfully inserted {len(df)} rows into {table_id}")
              except Exception as e:
                  print(f"Error loading data to BigQuery: {e}")
                  import traceback
                  traceback.print_exc()
          else:
              print("No 'hourly' data found in the API response!")
              print("Available keys:", weather_data.keys())
      except json.JSONDecodeError as e:
          print(f"Error parsing body content as JSON: {e}")
          print("Raw body content:", body_content)




  - id: run_dbt
    type: io.kestra.plugin.scripts.shell.Commands
    commands:
      - mkdir -p /tmp/dbt_logs
      - cd /app/workflows/dbt_project/dbt_weather && dbt run --profiles-dir /app/.dbt --log-path /tmp/dbt_logs --target prod




*********************************
********************************
CALISAN 09/04/2025  14:00  FULL KOD
ps. her zaman bashte dbt run testini yap eger hata olursa!!


id: weather_daily_pipeline
namespace: weather
description: "Daily weather data pipeline for Helsinki using archive API"


variables:
  api_base_url: "https://api.open-meteo.com/v1/archive"
  location_latitude: "60.1695"  # Helsinki
  location_longitude: "24.9354"  # Helsinki
  timezone: "Europe/Berlin"
  hourly_params: "temperature_2m,rain,snowfall,weather_code,soil_temperature_0_to_7cm,relative_humidity_2m,precipitation"
  gcp_project: "forward-dream-451119-q3"
  bigquery_dataset: "weather_dataset"
  bigquery_raw_table: "weather_data"


tasks:
  - id: calculate_date_range
    type: io.kestra.plugin.scripts.python.Script
    description: "Calculate start date for API call (fetch yesterday's data)"
    runner: PROCESS
    script: |
      import datetime
      from kestra import Kestra

      today = datetime.datetime.now()
      yesterday = today - datetime.timedelta(days=1)
      #before_yesterday=yesterday-datetime.timedelta(days=1)
      #start_date = before_yesterday.strftime('%Y-%m-%d')  # '2025-04-07' gibi
      #end_date=yesterday.strftime('%Y-%m-%d')
      search_date= yesterday.strftime('%Y-%m-%d')

      # we put the variables to output
      Kestra.outputs({
          "search_day": search_day
          #"start_date": start_date,
          #"end_date": end_date
      })


  - id: print_output
    type: io.kestra.plugin.core.log.Log
    message: |
      Value from Python: {{ outputs.calculate_date_range.vars.start_date }}

  - id: fetch_weather_data
    type: io.kestra.plugin.core.http.Request
    description: "Fetch weather data using dynamic start_date"
    uri: "https://archive-api.open-meteo.com/v1/archive?start_date={{ outputs.calculate_date_range.vars.search_day }}&end_date={{ outputs.calculate_date_range.vars.search_day }}&latitude={{ vars.location_latitude }}&longitude={{ vars.location_longitude }}&hourly={{ vars.hourly_params }}&timezone={{ vars.timezone }}"
    method: GET


  - id: transform_and_load_to_bigquery
    type: io.kestra.plugin.scripts.python.Script
    description: "Transform API response and load to BigQuery"
    runner: PROCESS 
    inputFiles:
      fetch_weather_data.json: "{{ outputs.fetch_weather_data }}"
    env:
      GOOGLE_APPLICATION_CREDENTIALS: /app/.dbt/my-creds.json
    beforeCommands:
     - pip install google-cloud-bigquery pandas-gbq --upgrade  --no-warn-script-location pyarrow --no-cache-dir
    script: |
      import json
      import pandas as pd
      from google.cloud import bigquery
      import os
      from datetime import datetime
      
      # Dosyadan API yanıtını oku
      with open('fetch_weather_data.json', 'r') as f:
          content = f.read()
      
      # API yanıtını doğrudan yazdır
      print("Raw API response:", content[:200] + "..." if len(content) > 200 else content)
      
      # JSON'ı parse et
      response_data = json.loads(content)
      
      # Body içeriğini al
      body_content = response_data.get('body', '{}')
      
      # Body içeriğini yazdır
      print("Body content (first 200 chars):", body_content[:200] + "..." if len(body_content) > 200 else body_content)
      
      # Body içeriğini JSON olarak parse et
      try:
          weather_data = json.loads(body_content)
          print("Weather data keys:", weather_data.keys())
          
          # Hourly verisi var mı kontrol et
          if 'hourly' in weather_data:
              print("Hourly data keys:", weather_data['hourly'].keys())
              print("Time data length:", len(weather_data['hourly']['time']))
              
              # Veriyi çıkar
              times = weather_data['hourly']['time']
              temperature = weather_data['hourly']['temperature_2m']
              rain = weather_data['hourly']['rain']
              snowfall = weather_data['hourly']['snowfall']
              weather_code = weather_data['hourly']['weather_code']
              soil_temp = weather_data['hourly']['soil_temperature_0_to_7cm']
              rel_humidity = weather_data['hourly']['relative_humidity_2m']
              precipitation = weather_data['hourly']['precipitation']
              
              # DataFrame oluştur
              df = pd.DataFrame({
                  "date_str": times,
                  "temperature_2m": temperature,
                  "rain": rain,
                  "snowfall": snowfall,
                  "weather_code": weather_code, 
                  "soil_temperature_0_to_7cm": soil_temp,
                  "relative_humidity_2m": rel_humidity,
                  "precipitation": precipitation
              })
              
              print("DataFrame created with shape:", df.shape)
              print("DataFrame head:")
              print(df.head().to_string())
              
              # Tarihleri uygun formata dönüştür
              df['date'] = pd.to_datetime(df['date_str'])
              df = df.drop(columns=['date_str'])
              
              # Veri tipi dönüşümlerini kontrol et
              #df['weather_code'] = df['weather_code'].astype(int)
              
              # BigQuery config
              client = bigquery.Client(project="forward-dream-451119-q3")
              table_id = "forward-dream-451119-q3.weather_dataset.weather_data"
              
              # Define schema for BigQuery table
              schema = [
                  bigquery.SchemaField("date", "TIMESTAMP"),
                  bigquery.SchemaField("temperature_2m", "FLOAT"),
                  bigquery.SchemaField("rain", "FLOAT"),
                  bigquery.SchemaField("snowfall", "FLOAT"),
                  bigquery.SchemaField("weather_code", "FLOAT"),
                  bigquery.SchemaField("soil_temperature_0_to_7cm", "FLOAT"),
                  bigquery.SchemaField("relative_humidity_2m", "FLOAT"),
                  bigquery.SchemaField("precipitation", "FLOAT")
              ]
              
              # BigQuery job configuration
              job_config = bigquery.LoadJobConfig(
                  write_disposition="WRITE_APPEND",
                  schema=schema  # Applying schema for proper column mapping
              )
              
              try:
                  # Load DataFrame to BigQuery
                  job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
                  job_result = job.result()  # Wait for the job to complete
                  print(f"Successfully inserted {len(df)} rows into {table_id}")
              except Exception as e:
                  print(f"Error loading data to BigQuery: {e}")
                  import traceback
                  traceback.print_exc()
          else:
              print("No 'hourly' data found in the API response!")
              print("Available keys:", weather_data.keys())
      except json.JSONDecodeError as e:
          print(f"Error parsing body content as JSON: {e}")
          print("Raw body content:", body_content)
 

  - id: run_dbt
    type: io.kestra.plugin.scripts.shell.Commands
    runner: PROCESS 
    commands:
      - cd /app/workflows/dbt_project
      - dbt run --profiles-dir /app/.dbt --log-path /tmp/dbt_logs --target prod
      

triggers:
  - id: daily_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 0 * * *"  # Her gün sabah 06:00'da çalıştır (önceki günün tamamlanmış verileri için)
